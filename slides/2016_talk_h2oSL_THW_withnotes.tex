\documentclass[12pt,t,handout]{beamer}
\usepackage{graphicx}
\setbeameroption{show notes}
\setbeamertemplate{note page}[plain]
\usepackage{listings}
\usepackage{url}

\def\notescolors{1}
\input{header.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% end of header
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% title info
\title{A Practical Tour of Super Learning}
\author{Nima Hejazi \inst{1} \and Evan Muzzall \inst{2}}
\institute{\inst{1} Division of Biostatistics, University of
                    California, Berkeley
           \and
           \inst{2} D-Lab, University of California, Berkeley}

\date{ \underline{slides}: \url{https://goo.gl/wWa9QC}}
\begin{document}

% title slide
{
\setbeamertemplate{footline}{} % no page number here
\frame{
  \titlepage

  \vfill \hfill \includegraphics[height=6mm]{Figs/cc-zero.png} \vspace*{-1cm}

  \note{These are slides from a presentation on practical ensemble learning with
    the Super Learner and h2oEnsemble packages for the R language, most
    recently presented at a meeting of The Hacker Within, at the Berkeley
    Institute for Data Science at UC Berkeley, on 6 December 2016.

    source: {\tt https://github.com/nhejazi/talk-h2oSL-THW-2016} \\
    slides: {\tt https://goo.gl/CXC2FF} \\
    with notes: {\tt http://goo.gl/wWa9QC}
}
}
}


\begin{frame}[fragile,c]{1. Ensemble Learning -- What?}

\begin{center}
\begin{minipage}[c]{9.3cm}
\begin{semiverbatim}
\lstset{basicstyle=\normalsize}
\begin{lstlisting}[linewidth=10.0cm]
 In statistics and machine learning,
 ensemble methods use multiple
 learning algorithms to obtain better
 predictive performance than could be
 obtained from any of the constituent
 learning algorithms alone.

 - Wikipedia, November 2016
\end{lstlisting}
\end{semiverbatim}
\end{minipage}
\end{center}

\note{
  This rather elementary definition of ``ensemble learning'' encapsulates quite
  well the core notions necessary to understand why we might be interested in
  optimizing such procedures. In particular, we will see that a weighted
  collection of individual learning algorithms can not only outperform other
  algorithms in practice but also has been shown to be theoretically optimal.
}
\end{frame}


\begin{frame}[c]{2. Ensemble Learning -- Why?}

\vspace*{3mm}

\centering

  \begin{itemize}
    \itemsep12pt
    \item Ensemble methods outperform individual (base) learning algorithms.
    \item By combining a set of individual learning algorithms using a
          \textit{metalearning} algorithm, ensemble methods can approximate
          complex functional relationships.
    \item When the true functional relationship is not in the set of base
          learning algorithms, ensemble methods approximate the function well.
    \item \textit{n.b.}, ensemble methods can, even asymptotically, perform only
          as well as the best weighted combination of the candidate learners.
  \end{itemize}
\note{
  A variety of techniques exist for ensemble learning, ranging from the classic
  ``random forest'' (of Leo Breiman) to ``xgboost'' to ``Super Learner'' (van
  der Laan \textit{et al.}). In this talk, we will focus on the elementary
  theoretical properties of ``Super Learner'', with an eye towards application.

  Theoretically, a range of different algorithms can be usedin the metalearning
  step; however, in practice, often, logistic regression is used.
}
\end{frame}


\begin{frame}[c]{3. Ensemble Learning -- How?}

\vspace*{3mm}

\textit{Common strategies for performing ensemble learning:}

\vspace{1em}

\centering

  \begin{itemize}
    \itemsep12pt
    \item \textbf{Bagging} -- reduces variance and increases accuracy; robust
          against outliers; often used with decision trees (\textit{e.g.},
          Random Forest).
    \item \textbf{Boosting} -- reduces variance and increases accuracy; not
          robust against outliers or noise; accomodates any loss function.
    \item \textbf{Stacking} -- used in combining ``strong'' learners; requires
          a \textit{metalearning} algorithm to combine the set of learners.
  \end{itemize}

\note{
}
\end{frame}


\begin{frame}[c]{4. Introduction to Super Learner}

\vspace*{3mm}

\centering

  \begin{itemize}
    \itemsep12pt
    \item 1996 paper ``Stacked Regressions'' (L. Breimen) introduced the notion
          of model stacking using k-fold cross-validation, the precursor to the
          modern Super Learner algorithm.
    \item 2007 paper ``Super Learner'' (van der Laan \textit{et al.}) worked out
          theoretical details on the optimality of stacking. Before this, the
          reasons for the superb performance of stacking were unknown.
    \item The Super Learner algorithm learns the optimal combination of the
          base learner fits. The above paper proved the asymptotic optimality
          of this strategy.
  \end{itemize}

\note{
}
\end{frame}


\begin{frame}[c]{5. Interlude: Cross-Validation}

\vspace*{3mm}

\centering

\includegraphics[scale=0.45]{Figs/cv.pdf}

\note{
}
\end{frame}


\begin{frame}[c]{6. Optimality of Super Learner}

\vspace*{3mm}

\centering

  \begin{itemize}
    \itemsep12pt
    \item j
    \item j
    \item j
  \end{itemize}

\note{
}
\end{frame}


\begin{frame}[c]{7. The Super Learner Algorithm}

\vspace*{3mm}

\centering

\includegraphics[scale=0.4]{Figs/sl.pdf}

\note{
  \textit{Cross-validate Base Learners:}
    \begin{itemize}
      \itemsep12pt
      \item Perform k-fold cross-validation on each of these learners and
            collect the cross-validated predicted values from each of the $L$
            algorithms.
      \item The $N$ cross-validated predicted values from each of the $L$
            algorithms can be combined to form a new $N \times L$ matrix. This
            matrix, along wtih the original response vector, is called the
            "level-one" data.
    \end{itemize}

  \textit{Metalearning:}
    \begin{itemize}
      \itemsep12pt
      \item Train the metalearning algorithm on the level-one data.
      \item Train each of the $L$ base algorithms on the full training set.
      \item The "ensemble model" consists of the $L$ base learning models and
            the metalearning model, which can then be used to generate
            predictions on a test set.
    \end{itemize}
}
\end{frame}


\begin{frame}[c]{8. R Package: ``SuperLearner''}

\vspace*{3mm}

\centering

  \begin{itemize}
    \itemsep12pt
    \item Implements the Super Learner prediction method (stacking) and
          contains a library of prediction algorithms to be used in the Super
          Learner.
    \item Provides a clean interface to numerous algorithms in R and defines a
          consistent API for extensibility.
  \end{itemize}

\note{
}
\end{frame}


\begin{frame}{9. R Package: ``h2oEnsemble''}

\vspace*{3mm}

\centering

\textit{Extension to the ``h2o'' R package that allows the user to train an
ensemble in the H2O cluster using any of the supervised machine learning
algorithms in H2O.}

\vspace{1em}

  \begin{itemize}
    \itemsep12pt
    \item Uses data-distributed and parallelized Java-based algorithms for the
          ensemble.
    \item All training and data processing are performed in the
          high-performance H2O cluster.
    \item Supports regression and binary classification.
  \end{itemize}

\note{
}
\end{frame}


\begin{frame}[c]{Summary}

  \begin{enumerate}
  \itemsep12pt
    \item Ensemble methods combine individual learning algorithms to approximate
          complex relationships.
    \item Super Learning (stacking) represents an optimal system for combining
          individual learning algorithms into an ensemble learner.
    \item The ``SuperLearner'' R package provides a well-maintained
          implementation of the the Super Learner algorithm.
    \item The ``h2oEnsemble'' R package provides access to a range of ensemble
          methods, developed by \href{http://www.h2o.ai}{\tt H2O.ai}.
  \end{enumerate}

  \note{
    Just a summary of what we discussed today.
}
\end{frame}


\begin{frame}[c]{}

\Large

Slides: \url{http://goo.gl/wWa9QC} \quad
\includegraphics[height=5mm]{Figs/cc-zero.png}

\vspace{10mm}

\href{https://github.com/nhejazi/talk-h2oSL-THW-2016}
{\tt GitHub: nhejazi/talk-h2oSL-THW-2016}

\note{
  Here's where you can find the resources prepared for this talk.
}
\end{frame}


\end{document}
