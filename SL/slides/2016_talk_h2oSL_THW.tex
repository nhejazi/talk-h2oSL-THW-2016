\documentclass[12pt,t]{beamer}
\usepackage{graphicx}
\setbeameroption{hide notes}
\setbeamertemplate{note page}[plain]
\usepackage{listings}
\usepackage{url}

\graphicspath{ {Figs/} }
\input{header.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% end of header
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% title info
\title{A \textit{Practical} Tour of Ensemble (Machine) Learning}
\author{Nima Hejazi \inst{1} \and Evan Muzzall \inst{2}}
\institute{\inst{1} Division of Biostatistics, University of
                    California, Berkeley
           \and
           \inst{2} D-Lab, University of California, Berkeley}

\date{ \underline{slides}: \url{https://goo.gl/wWa9QC}}
\begin{document}

% title slide
{
\setbeamertemplate{footline}{} % no page number here
\frame{
  \titlepage

  \vfill \hfill \includegraphics[height=6mm]{cc-zero.png} \vspace*{-1cm}

  \note{These are slides from a presentation on practical ensemble learning with
    the Super Learner and h2oEnsemble packages for the R language, most
    recently presented at a meeting of The Hacker Within, at the Berkeley
    Institute for Data Science at UC Berkeley, on 6 December 2016.

    source: {\tt https://github.com/nhejazi/talk-h2oSL-THW-2016} \\
    slides: {\tt https://goo.gl/CXC2FF} \\
    with notes: {\tt http://goo.gl/wWa9QC}
}
}
}


\begin{frame}[fragile,c]{Ensemble Learning -- What?}

\begin{center}
\begin{minipage}[c]{9.3cm}
\begin{semiverbatim}
\lstset{basicstyle=\normalsize}
\begin{lstlisting}[linewidth=10.0cm]
 In statistics and machine learning,
 ensemble methods use multiple
 learning algorithms to obtain better
 predictive performance than could be
 obtained from any of the constituent
 learning algorithms alone.

 - Wikipedia, November 2016
\end{lstlisting}
\end{semiverbatim}
\end{minipage}
\end{center}

\note{
  This rather elementary definition of ``ensemble learning'' encapsulates quite
  well the core notions necessary to understand why we might be interested in
  optimizing such procedures. In particular, we will see that a weighted
  collection of individual learning algorithms can not only outperform other
  algorithms in practice but also has been shown to be theoretically optimal.
}
\end{frame}


\begin{frame}[c]{Ensemble Learning -- Why?}

\vspace*{3mm}

\centering

  \begin{itemize}
    \itemsep12pt
    \item Ensemble methods outperform individual (base) learning algorithms.
    \item By combining a set of individual learning algorithms using a
          \textit{metalearning} algorithm, ensemble methods can approximate
          complex functional relationships.
    \item When the true functional relationship is not in the set of base
          learning algorithms, ensemble methods approximate the true function
          well.
    \item \textit{n.b.}, ensemble methods can, even asymptotically, perform only
          as well as the best weighted combination of the candidate learners.
  \end{itemize}
\note{
  A variety of techniques exist for ensemble learning, ranging from the classic
  ``random forest'' (of Leo Breiman) to ``xgboost'' to ``Super Learner'' (van
  der Laan \textit{et al.}). In this talk, we will focus on the elementary
  theoretical properties of ``Super Learner'', with an eye towards application.

  Theoretically, a range of different algorithms can be used in the metalearning
  step; however, in practice, often, logistic regression is used.
}
\end{frame}


\begin{frame}[c]{Ensemble Learning -- How?}

\vspace*{3mm}

\textit{Common strategies for performing ensemble learning:}

\vspace{1em}

\centering

  \begin{itemize}
    \itemsep12pt
    \item \underline{\textbf{Bagging}} -- reduces variance and increases
          accuracy; robust against outliers; often used with decision trees
          (\textit{i.e.}, Random Forest).
    \item \underline{\textbf{Boosting}} -- reduces variance and increases
          accuracy; not robust against outliers or noise; accomodates any loss
          function.
    \item \underline{\textbf{Stacking}} -- used in combining ``strong''
          learners; requires a \textit{metalearning} algorithm to combine the
          set of learners.
  \end{itemize}

\note{
  While a number of different strategies exist for combining various types of
  learning algorithms, most modern methods rely on stacking to produce powerful
  ensemble learners. These sorts of ensemble learners are what you want to use
  to win Kaggle competitions!
}
\end{frame}


\begin{frame}[c]{Introduction to Super Learner}

\vspace*{3mm}

\centering

  \begin{itemize}
    \itemsep12pt
    \item 1996 paper ``Stacked Regressions'' (L. Breiman) introduced the notion
          of model stacking using k-fold cross-validation, the precursor to the
          modern Super Learner algorithm.
    \item 2007 paper ``Super Learner'' (van der Laan \textit{et al.}) worked out
          theoretical details on the optimality of stacking. Before this, the
          reasons for the superb performance of stacking were unknown.
    \item The Super Learner algorithm learns the optimal combination of the
          base learner fits in a manner that is provably asymptotic optimal.
  \end{itemize}

\note{
  The Super Learner algorithm allows researchers to use multiple algorithms to
  outperform a single algorithm in realistic non-parametric and semi-parametric
  statistical models that are based on actual knowledge.

  The term algorithm is used very loosely to describe any mapping from the data
  into a predictor. This can range from a simple logistic regression to more
  complex algorithms such as neural nets.
}
\end{frame}


\begin{frame}[c]{Interlude: Cross-Validation}

\vspace*{2mm}

\centering

\includegraphics[scale=0.45]{cv.pdf}

\textit{The validation set rotates V times such that each set is used as the
validation set once.}

\note{
  Cross-validation solves the problem of having many algorithms, and not
  knowing which one to use and helps us avoid overfitting.

  For any given fold, $V âˆ’ 1$ sets will comprise the training set and the
  remaining $1$ set is the validation set.

  The observations in the training set are used to construct (or train) the
  candidate estimators.

  The observations in the validation set are used to assess the performance
  (\textit{i.e.}, risk) of the candidate algorithms.
}
\end{frame}


\begin{frame}[c]{Optimality of Super Learner}

\vspace*{1mm}

\centering

  For a random variable $O = (W, A, Y)$, let the \textbf{oracle selector} be a
  rule that picks the algorithm with the lowest cross-validated risk under the
  \textit{true probability distribution} $P_{0}$. The \textbf{oracle selector}
  is unknown because it depends on observed data \underline{and} the truth.

\vspace{1em}

  Asymptotic results prove that in realistic scenarios (where none of the
  algorithms represent the true relationship), the ``discrete super learner''
  performs \textit{asymptotically as well as} the oracle selector (the best
  estimator given the algorithms in the collection).

\note{
  To clarify, theory shows that that the discrete super learner performs as
  well as the oracle selector, up to a second order term. The loss function
  must be bounded, and then we will perform as well as the algorithm that is
  the risk minimizer of the expected loss function. The number of algorithms in
  the library can grow with sample size.
}
\end{frame}


\begin{frame}[c]{The Discrete Super Learner}

\vspace*{3mm}

\centering

\includegraphics[scale=0.45]{discreteSL.pdf}

\note{
}
\end{frame}


\begin{frame}[c]{The Super Learner Algorithm}

\vspace*{3mm}

\centering

\includegraphics[scale=0.4]{SL.pdf}

\note{
  \underline{1. Cross-validate base learners}:
    \begin{itemize}
      \itemsep10pt
      \item Perform k-fold cross-validation on each learner and collect the
            cross-validated predicted values from each of the $L$ algorithms.
      \item The $N$ cross-validated predicted values from each of the $L$
            algorithms can be combined to form a new $N \times L$ matrix. Call
            this matrix, with the original response vector, ``level-one'' data.
    \end{itemize}

  \underline{2. Metalearning}:
    \begin{itemize}
      \itemsep10pt
      \item Train the metalearning algorithm on the ``level-one'' data.
      \item Train each of the $L$ base algorithms on the full training set.
      \item The ``ensemble model'' consists of the $L$ base learning models and
            the metalearning model, which can be used to generate predictions on
            a test set.
    \end{itemize}
}
\end{frame}


\begin{frame}[c]{Ensembles with Super Learner}

\vspace*{3mm}

\centering

\includegraphics[scale=0.55]{optimalSL.png}

\note{
}
\end{frame}


\begin{frame}[c]{R Package: ``SuperLearner''}

\vspace*{3mm}

\centering

  \begin{itemize}
    \itemsep12pt
    \item Implements the Super Learner prediction method (stacking) and
          contains a library of prediction algorithms to be used in the Super
          Learner.
    \item Provides a clean interface to numerous algorithms in R and defines a
          consistent API for extensibility.
  \end{itemize}

\note{
}
\end{frame}


\begin{frame}{R Package: ``h2oEnsemble''}

\vspace*{3mm}

\centering

\textit{Extension to the ``h2o'' R package that allows the user to train an
ensemble in the H2O cluster using any of the supervised machine learning
algorithms in H2O.}

\vspace{1em}

  \begin{itemize}
    \itemsep12pt
    \item Uses data-distributed and parallelized Java-based algorithms for the
          ensemble.
    \item All training and data processing are performed in the
          high-performance H2O cluster.
    \item Supports regression and binary classification.
  \end{itemize}

\note{
}
\end{frame}


\begin{frame}[c]{Summary}

  \begin{enumerate}
  \itemsep12pt
    \item Ensemble methods combine individual learning algorithms to approximate
          complex relationships.
    \item Super Learning (stacking) represents an optimal system for combining
          individual learning algorithms into an ensemble learner.
    \item The ``SuperLearner'' R package provides a well-maintained
          implementation of the the Super Learner algorithm.
    \item The ``h2oEnsemble'' R package provides access to a range of ensemble
          methods, developed by \href{http://www.h2o.ai}{\tt H2O.ai}.
  \end{enumerate}

  \note{
    Just a summary of what we discussed today.
}
\end{frame}


\begin{frame}[c]{}

\Large

Slides: \url{http://goo.gl/wWa9QC} \quad
\includegraphics[height=5mm]{cc-zero.png}

\vspace{15mm}

GitHub: \href{https://github.com/nhejazi/talk-h2oSL-THW-2016}
{\tt nhejazi/talk-h2oSL-THW-2016}

\note{
  Here's where you can find the resources prepared for this talk.
}
\end{frame}


\end{document}
