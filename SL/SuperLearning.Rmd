---
title: "Ensemble (Machine) Learning with SuperLearner and H2O in R"
author: "Nima Hejazi and Evan Muzzall"
date: "12/6//2016"
output: 
  html_document: 
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1.  Preliminaries

* In this walkthrough, we will examine how to use the `SuperLearner` R package
by looking at the BreastCancer data set from the R package `mlbench`.

* For convenience, the `BreastCancer` dataset is also available in CSV format in
the data subdirectory of this repo.

```{r}
set.seed(654123) # reproducibility matters
"%ni%" = Negate("%in%")

# load some standard packages
library(dplyr)
library(ggplot2)
library(mlbench)

# load the data set
data(BreastCancer)
head(BreastCancer)
```

# 2.  Data Cleaning

* Firstly, we need to transform our data matrix so that it can be easily passed
to `SuperLearner`.

* We'll do this with the `dplyr` package, which we loaded in the above section.

```{r}
# examine whether there are NAs in the data
colSums(is.na(BreastCancer))
```

* `SuperLearner` __does not work with missing data__, so we will have to remove 
incomplete observations.

* There are several ways to handle missing data via imputation with
`SuperLearner`, but those are outside the scope of this tutorial. Feel free to
ask us after this workshop if you're interested in this.

```{r}
# remove the NAs before proceeding with Super Learner
bc <- BreastCancer %>%
 dplyr::filter(complete.cases(.))
colSums(is.na(bc))
```

* Next, we'll need to remove the outcome column ("Class") and the metadata
column ("ID") from the X matrix

* Here, the X matrix is simply the observed values to be used in predicting the
class outcome.

```{r}
# create a data.frame of covariates to be used in prediction
X <- bc %>%
 dplyr::select(which(colnames(.) %ni% c("Id", "Class")))
head(X)
```

* We simply strip all columns other than the "Class" to create the outcome
vector.

* The vector of outcomes must be of class numeric for SuperLearner to work
properly.

```{r}
# create a numeric vector of the binary outcomes.
Y <- bc %>%
 dplyr::select(which(colnames(.) %in% c("Class")))
Y <- as.vector(ifelse(Y == "benign", 0, 1))
unique(Y)
```

# 3.  The Super Learner Algorithm

* __R package:__ `SuperLearner`

* Main functions: `SuperLearner`, `CV.SuperLearner`

To start, we will load the `SuperLearner` package and set up a library of
learning algorithms that `SuperLearner` will combine into a single ensemble
learner.

For the purposes of this walkthrough, we'll keep the library of learning
algorithms limited. Note that in practical cases, the number of learning
algorithms should vary with the number of observations, and the bigger your
library, the better you will capture the optimal relationship.

```{r}
library(SuperLearner)

SL_library <- c("SL.gbm", "SL.glm", "SL.glmnet", "SL.knn", "SL.nnet", 
                "SL.randomForest")
```

```{r}
SL_fit <- SuperLearner(X = X,
                       Y = Y,
                       family = binomial(),
                       SL.library = SL_library,
                       verbose = FALSE
                      )
```

```{r}
SL_fit$coef
```

# 4.  The Cross-Validated Super Learner

* "Function to get V-fold cross-validated risk estimate for super learner. This 
function simply splits the data into V folds and then calls `SuperLearner`."

* Why would we want to cross-validate the ensembling process used by Super
Learner?
  * The `SuperLearner` function builds an estimator, but does not contain an 
    estimate of the performance of that estimator.
  * The authros recommend using cross-validation to evaluate the honest
    performance of the super learner estimator.

* The function `CV.SuperLearner` computes the usual V-fold cross-validated risk 
estimate for the super learner (as well as that for each algorithm in the
specified library for comparison).

```{r}
library(cvAUC)

V = 5 # we'll use few folds here to save time

cv_SL_fit <- CV.SuperLearner(X = X,
                             Y = Y,
                             V = V,
                             family = binomial(),
                             SL.library = SL_library,
                             verbose = FALSE
                            )
```

```{r}
cv_SL_fit$coef
```

```{r}
plot(cv_SL_fit)
```

```{r}
fold = cv_SL_fit$fold
predsY = cv_SL_fit$SL.predict

n = length(predsY)
folds = rep(NA, n)

for(k in 1:V) {
    ii = unlist(fold[k])
    folds[ii] = k
}

ci_out = ci.cvAUC(predsY, Y, folds = folds)
txt = paste("AUC = ", round(ci_out$cvAUC, 2),",
            95% CI = ", round(ci_out$ci[1], 2), "-",
            round(ci_out$ci[2], 2))
```

# 5.  Evaluating Classification Results

* The [Receiver Operating Characteristic
curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) is a 
standard way of evaluating the results of classification algorithms

* Here, we use the ROCR package to visualize the results from the
cross-validated Super Learner

```{r}
library(ROCR)

pred <- prediction(predsY, Y)
perf <- performance(pred, "sens", "spec")
plot(1 - slot(perf, "x.values")[[1]],
     slot(perf, "y.values")[[1]],
     type = "s",
     xlab = "1 - Specificity",
     ylab = "Sensitivity",
     main = "Receiver Operating Characteristic (ROC) Curve")
text(0.75, 0.1, txt)
abline(0, 1)
```
